# ğŸ§  MCPé–¢é€£ã‚’1ã‹ã‚‰ä½œã£ã¦ã¿ãŸ

## æ¦‚è¦

ãƒŸãƒ‹ãƒãƒ ãªMCPï¼ˆModel Context Protocolï¼‰å½¢å¼ã®LLM + Tool Calling ã‚µãƒ³ãƒ—ãƒ«ã€‚

- vLLM API çµŒç”±ã§ LLM ã‚’å‘¼ã³å‡ºã™
- JSONå‡ºåŠ›ã«åŸºã¥ã FastAPI ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ¼ãƒã§é–¢æ•°ã‚’å‘¼ã³å‡ºã™

## ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆ

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ llm/                      # LLMã¨ã®ã‚„ã‚Šã¨ã‚Šã«é–¢ã™ã‚‹ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ llm_wrapper.py       # vLLMã®OpenAIäº’æ›APIã‚’å‘¼ã³å‡ºã™ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”‚   â””â”€â”€ prompt_template.txt  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆï¼ˆãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã®æŒ‡ç¤ºä¾‹ãªã©ï¼‰
â”‚
â”œâ”€â”€ mcp_client/               # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå®Ÿè£…
â”‚   â”œâ”€â”€ client.py            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®å…¥åŠ›ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›â†’LLMâ†’çµæœã‚’APIé€ä¿¡
â”‚   â””â”€â”€ main.py              # JSONå‡ºåŠ›ã‚’ç›´æ¥ãƒ­ãƒ¼ã‚«ãƒ«ã§å‡¦ç†ï¼ˆã‚µãƒ¼ãƒã‚’çµŒç”±ã—ãªã„ãƒ†ã‚¹ãƒˆç”¨ï¼‰
â”‚
â””â”€â”€ mcp_server/               # FastAPIãƒ™ãƒ¼ã‚¹ã®MCPã‚µãƒ¼ãƒ
     â”œâ”€â”€ server.py            # /tool-call ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå®šç¾©
     â”œâ”€â”€ functions.py         # å®Ÿè¡Œå¯¾è±¡ã¨ãªã‚‹ãƒ„ãƒ¼ãƒ«é–¢æ•°ã®å®Ÿä½“
     â””â”€â”€ function_registry.py # ãƒ„ãƒ¼ãƒ«åâ†’é–¢æ•°ã®ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¨å®‰å…¨ãªå‘¼ã³å‡ºã—ãƒ©ãƒƒãƒ‘ãƒ¼
```

## å®Ÿè¡Œæ–¹æ³•

### 1. .envãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š

ãƒ­ãƒ¼ã‚«ãƒ«ã§ `vLLM` ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã™ã‚‹éš›ã€`llm/llm_wrapper.py` ã¯ `VLLM_API_BASE` ç’°å¢ƒå¤‰æ•°ã‚’å‚ç…§ã—ã¾ã™ã€‚`.env` ã§è¨­å®šã—ã¦ãŠãã¾ã™ã€‚

```bash
echo 'VLLM_API_BASE=http://localhost:8000/v1' > .env
```

### 2. vLLMã‚µãƒ¼ãƒèµ·å‹•
åˆ¥é€”vLLMã‚µãƒ¼ãƒã‚’ `--model facebook/opt-125m` ãªã©ã§èµ·å‹•ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

ä¾‹ï¼š
```bash
python3 -m vllm.entrypoints.openai.api_server --model facebook/opt-125m
```

### 3. FastAPIã‚µãƒ¼ãƒã‚’Dockerã§èµ·å‹•
```bash
docker build -t mcp-app .
docker run -it -p 8000:8000 mcp-app
```

### 4. ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆèµ·å‹•ï¼ˆãƒ›ã‚¹ãƒˆå´ã§ï¼‰
```bash
python3 mcp_client/client.py
```

### 5. main.pyã‚’å®Ÿè¡Œ
`mcp_client/main.py` ã¯FastAPIã‚’ä½¿ã‚ãšã€ç›´æ¥é–¢æ•°å‘¼ã³å‡ºã—ã—ã¾ã™ã€‚

```bash
python3 mcp_client/main.py
```

## å¿…è¦è¦ä»¶
- vLLM serverï¼ˆåˆ¥é€”èµ·å‹•ãŒå¿…è¦ï¼‰
- Docker
- CUDA 12.1å¯¾å¿œGPUç’°å¢ƒï¼ˆä¾‹ï¼šRTX 3090, VRAM 24GBï¼‰

## å‚™è€ƒ

- prompt_template.txt ã«Tool Callã®ä¾‹ã‚’è¨˜è¼‰ã™ã‚‹ã¨ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ç²¾åº¦ãŒä¸ŠãŒã‚Šã¾ã™ã€‚
- JSONãŒå£Šã‚Œã‚‹å ´åˆã¯ `temperature` ã‚’ä¸‹ã’ã‚‹ã€ `max_tokens` ã‚’åˆ¶é™ã™ã‚‹ãªã©èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
